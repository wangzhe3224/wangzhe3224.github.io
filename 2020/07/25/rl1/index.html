<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Titillium Web:300,300italic,400,400italic,700,700italic|Encode Sans:300,300italic,400,400italic,700,700italic|Source Code Pro:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.css">
<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Gemini","version":"7.7.2","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":true,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":true,"mediumzoom":false,"lazyload":true,"pangu":false,"comments":{"style":"tabs","active":"disqus","storage":true,"lazyload":false,"nav":null,"activeClass":"disqus"},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Reinforcement learning, RL is a framework that let an agent to make suitable  decisions to achieve best goal. Underneath math problem to solve is a  Markov Decision Process, MDP.  RL is different fro">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforment Learning Introduction 1 - 2">
<meta property="og:url" content="http://yoursite.com/2020/07/25/rl1/index.html">
<meta property="og:site_name" content="Zhe">
<meta property="og:description" content="Reinforcement learning, RL is a framework that let an agent to make suitable  decisions to achieve best goal. Underneath math problem to solve is a  Markov Decision Process, MDP.  RL is different fro">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://hackmd.io/EF3LjEjfQtCa-yEZlFNCmQ/badge">
<meta property="article:published_time" content="2020-07-25T00:00:00.000Z">
<meta property="article:modified_time" content="2021-04-20T20:57:59.053Z">
<meta property="article:author" content="Zhe">
<meta property="article:tag" content="Reinforcement Learning">
<meta property="article:tag" content="n-arm bandit">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hackmd.io/EF3LjEjfQtCa-yEZlFNCmQ/badge">

<link rel="canonical" href="http://yoursite.com/2020/07/25/rl1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Reinforment Learning Introduction 1 - 2 | Zhe</title>
  
    <script>
      function sendPageView() {
        if (CONFIG.hostname !== location.hostname) return;
        var uid = localStorage.getItem('uid') || (Math.random() + '.' + Math.random());
        localStorage.setItem('uid', uid);
        navigator.sendBeacon('https://www.google-analytics.com/collect', new URLSearchParams({
          v  : 1,
          tid: 'UA-66629043-1',
          cid: uid,
          t  : 'pageview',
          dp : encodeURIComponent(location.pathname)
        }));
      }
      document.addEventListener('pjax:complete', sendPageView);
      sendPageView();
    </script>






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

  <script data-ad-client="ca-pub-9801985319378542" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
  <meta name="google-site-verification" content="MjVVxpOXJOpseO4Uw4kLKcO-jlZdu7grfbyyAbwnzG8" />
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=UA-66629043-1"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-66629043-1');
  </script>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Zhe</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Unstable Correlation</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-arts">

    <a href="/arts/" rel="section"><i class="fa fa-fw fa-microchip"></i>Arts</a>

  </li>
        <li class="menu-item menu-item-readings">

    <a href="/readings/" rel="section"><i class="fa fa-fw fa-archive"></i>Readings</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="reading-progress-bar"></div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/07/25/rl1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/lambda.jpeg">
      <meta itemprop="name" content="Zhe">
      <meta itemprop="description" content="Things are correlated">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Zhe">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reinforment Learning Introduction 1 - 2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-07-25 00:00:00" itemprop="dateCreated datePublished" datetime="2020-07-25T00:00:00Z">2020-07-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Quant/" itemprop="url" rel="index"><span itemprop="name">Quant</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Disqus: </span>
    
    <a title="disqus" href="/2020/07/25/rl1/#disqus_thread" itemprop="discussionUrl">
      <span class="post-comments-count disqus-comment-count" data-disqus-identifier="2020/07/25/rl1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>9.5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>9 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p><a target="_blank" rel="noopener" href="https://hackmd.io/EF3LjEjfQtCa-yEZlFNCmQ"><img data-src="https://hackmd.io/EF3LjEjfQtCa-yEZlFNCmQ/badge" alt="hackmd-github-sync-badge"></a></p>
<p>Reinforcement learning, RL is a framework that let an agent to make suitable  decisions to achieve best goal. Underneath math problem to solve is a  Markov Decision Process, MDP.  RL is different from both supervised and unsupervised learning. </p>
<h1 id="Elements-of-RL"><a href="#Elements-of-RL" class="headerlink" title="Elements of RL"></a>Elements of RL</h1><p>Apart from Agent and Environment, following elements also play central<br>roles: Policy, Reward Signal, Value Function, and Model of environment. </p>
<p>Policy, is a map from current states to actions to take. It might be<br>deterministic or stochastic.</p>
<p>Reword signal, defines the goal of RL. At each step, environment will<br>give agent a single number, a reward.</p>
<p>Value function, specifies what is good in the long run. The estimation<br>of value is in the central part of RL.</p>
<p>Model, is what the agent think the environment will behave. Basically by<br>building a model of env, the agent can do planning better. But model env<br>sometime is very hard.</p>
<p>Not all RL model need full set of above. But a good value function does<br>help to make a better decision.In the end, evolutionary and value function methods both search the space of policies, but learning a value function takes advantage of information available during the course of play.</p>
<h1 id="A-bit-history"><a href="#A-bit-history" class="headerlink" title="A bit history"></a>A bit history</h1><p>There are two threads of RL histories. One thread concerns learning by trial and error that started in the psychology of animal learning. The other thread concerns the problem of optimal control and its solution using value functions and dynamic programming. </p>
<p>Although the two threads have been largely independent, the exceptions revolve around a third, less distinct thread concerning temporal-difference methods</p>
<h1 id="Simplest-problem-Multi-arm-Bandits"><a href="#Simplest-problem-Multi-arm-Bandits" class="headerlink" title="Simplest problem: Multi-arm Bandits"></a>Simplest problem: Multi-arm Bandits</h1><p>The most important feature distinguishing reinforcement learning from other types of learning is that it uses training information that evaluates the actions taken rather than instructs by giving correct actions.</p>
<p>Consider the following learning problem. You are faced repeatedly with a choice among n different options, or actions. After each choice you receive a numerical reward chosen from a stationary probability distribution that depends on the action you selected. Your objective is to maximize the expected total reward over some time period, for example, over 1000 action selections, or time steps.</p>
<h2 id="Action-Value-method"><a href="#Action-Value-method" class="headerlink" title="Action Value method"></a>Action Value method</h2><p>Assume $q(a)$ is the true value of action a, and the estimation on t step is $Q_t(a)$. Then the simplest idea is average: </p>
<p>$$Q_t(a) = \frac {R_1+ R_2 + … + R_{N_t(a)}}{N_t(a)}$$</p>
<p>Once we have $Q_t(a)$, we can then select the action with highest estimated action value. The <em>greedy</em> action selection method can be written as</p>
<p>$$A_t = argmax Q_t(a)$$</p>
<p>Greedy means that action selection always <strong>exploits</strong> currently knowledge to max immediate reward. A simple alternative is to behave greedily most of the time, but, <strong>explore</strong> new actions sometimes.</p>
<h2 id="Incremental-Implementation"><a href="#Incremental-Implementation" class="headerlink" title="Incremental Implementation"></a>Incremental Implementation</h2><p>$$Q_{k+1} = Q_k + \frac {1}{k}[R_k - Q_k]$$</p>
<p>So esentially, update previous estimation with adjustment of new update.</p>
<p>$$NewEstimate \leftarrow OldEstimate + StepSize * [Target - OldEstimate]$$</p>
<h2 id="Nonstationary-Problem"><a href="#Nonstationary-Problem" class="headerlink" title="Nonstationary Problem"></a>Nonstationary Problem</h2><p>For non-stationary problem, it makes more sense to has more weights on recent result.</p>
<p>$$Q_{k+1} = Q_k + \alpha [R_k - Q_k]$$</p>
<p>$$Q_{k+1} = (1-\alpha)^kQ_1 + \sum_{i=1}^k\alpha(1-\alpha)^{k-i}R_i$$</p>
<h2 id="Upper-Confidence-Bound-Action-Selection"><a href="#Upper-Confidence-Bound-Action-Selection" class="headerlink" title="Upper-Confidence-Bound Action Selection"></a>Upper-Confidence-Bound Action Selection</h2><p>$$A_t = argmax_a\Big[Q_t(a) + c\sqrt{\frac{ln t}{N_t(a)}}\Big]$$</p>
<p>The idea here is to add exploration more wisely. So if an action is not selected for a long time, it is more likely to be selected, and if an action has been selected a lot of time, it is more likely to be selected (stick with optimal action, i.e. exploiate. )</p>
<h2 id="Gradient-Bandits"><a href="#Gradient-Bandits" class="headerlink" title="Gradient Bandits"></a>Gradient Bandits</h2><p>we can also learn a preference of each action, $H_t(a)$. The more preference, the more change to take that action. But preference is a relative value.</p>
<p>$$\pi_{t}(a) = P{A_t=a} = \frac {e^{H_t(a)}}{\sum_{b=1}^{n}e^{H_t(b)}}$$</p>
<p>So action is a softmax of preferences. We want to learn the preference of each actions.</p>
<p>Initially, all preference is 0.</p>
<p>So the learning/updating process is:</p>
<p>$$H_{t+1}(A_t) = H_t(A_t) + \alpha(R_t - \bar{R_t})(1 - \pi_{t}(A_t))$$</p>
<p>$$H_{t+1}(a) = H_t(a) - \alpha(R_t - \bar{R_t}\pi_t(a), \forall{a} \ne A_t$$</p>
<p>Above is a stochastic approximation to gradient ascent:</p>
<p>$$H_{t+1}(a) = H_t(a) + \alpha \frac {\partial {E[R_t]}} {\partial {H_t(a)}}$$</p>
<p>where, $E[R_t] = \sum_{b}\pi_t(b)q(b)$</p>
<h1 id="Different-Agents-of-Mult-arm-Bandits"><a href="#Different-Agents-of-Mult-arm-Bandits" class="headerlink" title="Different Agents of Mult-arm Bandits"></a>Different Agents of Mult-arm Bandits</h1><h2 id="Random-Agent"><a href="#Random-Agent" class="headerlink" title="Random Agent"></a>Random Agent</h2><p>The agent pick action randomly from action space, <code>number_of_arms</code>.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Random</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A random agent.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">  This agent returns an action between 0 and &#x27;number_of_arms&#x27;, </span></span><br><span class="line"><span class="string">  uniformly at random. The &#x27;previous_action&#x27; argument of &#x27;step&#x27;</span></span><br><span class="line"><span class="string">  is ignored.</span></span><br><span class="line"><span class="string">  &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_of_arms</span>):</span></span><br><span class="line">    self._number_of_arms = number_of_arms</span><br><span class="line">    self.name = <span class="string">&#x27;random&#x27;</span></span><br><span class="line">    self.reset()</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, previous_action, reward</span>):</span></span><br><span class="line">    <span class="keyword">return</span> np.random.randint(self._number_of_arms)</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<h2 id="Greedy-Agent"><a href="#Greedy-Agent" class="headerlink" title="Greedy Agent"></a>Greedy Agent</h2><p>The agent pick action that has most big expected value.</p>
<p>So pick action as following:</p>
<p>$$A_t = argmax Q_t(a)$$</p>
<p>Every step, update Q value of previous actions and counter of actions:</p>
<p>$$N(A_{t-1}) = N(A_{t-1}) + 1$$<br>$$Q(A_{t-1}) = Q(A_{t-1}) + \alpha(R_t - Q(A_{t-1}))$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Greedy</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_of_arms</span>):</span></span><br><span class="line">    self._number_of_arms = number_of_arms</span><br><span class="line">    self.name = <span class="string">&#x27;greedy&#x27;</span></span><br><span class="line">    self.reset()</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, previous_action, reward</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(previous_action) == <span class="built_in">type</span>(<span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">return</span> np.random.randint(self._number_of_arms)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="comment"># update values</span></span><br><span class="line">      self.N[previous_action] += <span class="number">1</span></span><br><span class="line">      lr = <span class="number">1.</span>/self.N[previous_action]</span><br><span class="line">      error = reward - self.Q[previous_action]</span><br><span class="line">      self.Q[previous_action] += lr*error</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># get new actions</span></span><br><span class="line">      <span class="keyword">return</span> np.argmax(self.Q)</span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.Q = np.zeros((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    self.N = np.zeros((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h2 id="epsilon-Greedy-Agent"><a href="#epsilon-Greedy-Agent" class="headerlink" title="$\epsilon$-Greedy Agent"></a>$\epsilon$-Greedy Agent</h2><p>The issue of pure greedy agent is that it may stuck in some false action and never explore. So we add a small change to select action which does not have best q value, but just to explore to gather more information.</p>
<p>The update process is as same as Greedy agent. But the way we choose actions changed:</p>
<p>$$A_t = argmax Q, rand &gt; \epsilon$$<br>$$A_t = random action, rand &lt;= \epsilon$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EpsilonGreedy</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_of_arms, epsilon=<span class="number">0.1</span></span>):</span></span><br><span class="line">    self._number_of_arms = number_of_arms</span><br><span class="line">    self._epsilon = epsilon</span><br><span class="line">    self.name = <span class="string">&#x27;epsilon-greedy epsilon:&#123;&#125;&#x27;</span>.<span class="built_in">format</span>(epsilon)</span><br><span class="line">    self.reset()</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, previous_action, reward</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(previous_action) == <span class="built_in">type</span>(<span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">return</span> np.random.randint(self._number_of_arms)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="comment"># update values</span></span><br><span class="line">      self.N[previous_action] += <span class="number">1</span></span><br><span class="line">      lr = <span class="number">1.</span>/self.N[previous_action]</span><br><span class="line">      error = reward - self.Q[previous_action]</span><br><span class="line">      self.Q[previous_action] += lr*error</span><br><span class="line"></span><br><span class="line">      <span class="comment"># get new actions</span></span><br><span class="line">      ra = <span class="built_in">bool</span>( np.random.random() &lt; self._epsilon )</span><br><span class="line">      <span class="keyword">return</span> (<span class="keyword">not</span> ra) * np.argmax(self.Q) + (ra) * np.random.randint(self._number_of_arms)</span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.Q = np.zeros((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    self.N = np.zeros((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line"><span class="comment">## UCB Agent</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UCB</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_of_arms</span>):</span></span><br><span class="line">    self._number_of_arms = number_of_arms</span><br><span class="line">    self.name = <span class="string">&#x27;ucb&#x27;</span></span><br><span class="line">    self.reset()</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, previous_action, reward</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(previous_action) == <span class="built_in">type</span>(<span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">return</span> np.random.randint(self._number_of_arms)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.t += <span class="number">1</span></span><br><span class="line">      <span class="comment"># update values</span></span><br><span class="line">      self.N[previous_action] += <span class="number">1</span></span><br><span class="line">      lr = <span class="number">1.</span>/self.N[previous_action]</span><br><span class="line">      error = reward - self.Q[previous_action]</span><br><span class="line">      self.Q[previous_action] += lr*error</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># here is the extra bit</span></span><br><span class="line">      U = np.sqrt(np.log(self.t)/<span class="number">1.</span>/(self.N+<span class="number">1.</span>))</span><br><span class="line">      </span><br><span class="line">      <span class="comment"># get new actions</span></span><br><span class="line">      <span class="keyword">return</span> np.argmax(self.Q+U)</span><br><span class="line">      </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.Q = np.zeros((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    self.N = np.zeros((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    self.t = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="Reinforce-Agent"><a href="#Reinforce-Agent" class="headerlink" title="Reinforce Agent"></a>Reinforce Agent</h2><p>Base line will not affect mean, but will change the variance of the estimation. </p>
<p>After we have the policy, we can sample an action from the policy.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Reinforce</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, number_of_arms, step_size=<span class="number">0.1</span>, baseline=<span class="literal">False</span></span>):</span></span><br><span class="line">    self._number_of_arms = number_of_arms</span><br><span class="line">    self._lr = step_size</span><br><span class="line">    self.name = <span class="string">&#x27;reinforce, baseline: &#123;&#125;&#x27;</span>.<span class="built_in">format</span>(baseline)</span><br><span class="line">    self._baseline = baseline</span><br><span class="line">    self.reset()</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, previous_action, reward</span>):</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">type</span>(previous_action) == <span class="built_in">type</span>(<span class="literal">None</span>):</span><br><span class="line">      <span class="keyword">return</span> np.random.randint(self._number_of_arms)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.t += <span class="number">1</span></span><br><span class="line">      self.all_reward += reward</span><br><span class="line">      <span class="keyword">if</span> self._baseline:</span><br><span class="line">        base = self.all_reward / (self.t * <span class="number">1.</span>)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        base = <span class="number">0</span></span><br><span class="line">      <span class="comment"># update preferences</span></span><br><span class="line">      <span class="comment"># this is H_a, reduce others preference</span></span><br><span class="line">      self.prob -= self._lr * (reward-base) * self.policy</span><br><span class="line">      <span class="comment"># this is H_A, increase current action preference</span></span><br><span class="line">      self.prob[previous_action] += self._lr * (reward-base)</span><br><span class="line">      x = self.prob</span><br><span class="line">      y = np.exp(x - np.<span class="built_in">max</span>(x))</span><br><span class="line">      self.policy = y / np.<span class="built_in">sum</span>(y)</span><br><span class="line">      <span class="comment"># get new actions</span></span><br><span class="line">      <span class="comment"># here we sample an action from the updated policy</span></span><br><span class="line">      <span class="keyword">import</span> bisect</span><br><span class="line">      acc = np.cumsum(self.policy)</span><br><span class="line">      <span class="keyword">return</span> bisect.bisect(acc, np.random.random())</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reset</span>(<span class="params">self</span>):</span></span><br><span class="line">    self.policy = np.ones((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)/self._number_of_arms</span><br><span class="line">    self.prob = np.zeros((self._number_of_arms,),dtype=<span class="string">&#x27;float32&#x27;</span>)</span><br><span class="line">    self.t = <span class="number">0</span></span><br><span class="line">    self.all_reward = <span class="number">0</span> </span><br></pre></td></tr></table></figure>

<h1 id="Full-RL-Problem"><a href="#Full-RL-Problem" class="headerlink" title="Full RL Problem"></a>Full RL Problem</h1><p>Above problem is not a full RL problem, because there is no association between action and different situations. Full RL problem needs to learn a policy that maps situations to actions.</p>
<h1 id="Bandit-Env-Code-Attached"><a href="#Bandit-Env-Code-Attached" class="headerlink" title="Bandit Env Code Attached"></a>Bandit Env Code Attached</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">BernoulliBandit</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">  <span class="string">&quot;&quot;&quot;A stationary multi-armed Bernoulli bandit.&quot;&quot;&quot;</span></span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, success_probabilities, success_reward=<span class="number">1.</span>, fail_reward=<span class="number">0.</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Constructor of a stationary Bernoulli bandit.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      success_probabilities: A list or numpy array containing the probabilities,</span></span><br><span class="line"><span class="string">          for each of the arms, of providing a success reward.</span></span><br><span class="line"><span class="string">      success_reward: The reward on success (default: 1.)</span></span><br><span class="line"><span class="string">      fail_reward: The reward on failure (default: 0.)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    self._probs = success_probabilities</span><br><span class="line">    self._number_of_arms = <span class="built_in">len</span>(self._probs)</span><br><span class="line">    self._s = success_reward</span><br><span class="line">    self._f = fail_reward</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">step</span>(<span class="params">self, action</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;The step function.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      action: An integer or tf.int32 that specifies which arm to pull.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">      A sampled reward according to the success probability of the selected arm.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Raises:</span></span><br><span class="line"><span class="string">      ValueError: when the provided action is out of bounds.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> action &lt; <span class="number">0</span> <span class="keyword">or</span> action &gt;= self._number_of_arms:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">&#x27;Action &#123;&#125; is out of bounds for a &#x27;</span></span><br><span class="line">                       <span class="string">&#x27;&#123;&#125;-armed bandit&#x27;</span>.<span class="built_in">format</span>(action, self._number_of_arms))</span><br><span class="line"></span><br><span class="line">    success = <span class="built_in">bool</span>(np.random.random() &lt; self._probs[action])</span><br><span class="line">    reward = success * self._s + (<span class="keyword">not</span> success) * self._f</span><br><span class="line">    <span class="keyword">return</span> reward</span><br></pre></td></tr></table></figure>
    </div>

    
    
    
      
  <div class="popular-posts-header">Related Posts</div>
  <ul class="popular-posts">
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2020/07/31/rl2/" rel="bookmark">Reinforment Learning 3 Markov Decision Process</a></div>
    </li>
    <li class="popular-posts-item">
      <div class="popular-posts-title"><a href="/2020/08/01/rl4/" rel="bookmark">Reinforment Learning 4 Dynamic Programming</a></div>
    </li>
  </ul>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
              <a href="/tags/n-arm-bandit/" rel="tag"># n-arm bandit</a>
          </div>
        <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
<ins class="adsbygoogle"
     style="display:block; text-align:center;"
     data-ad-layout="in-article"
     data-ad-format="fluid"
     data-ad-client="ca-pub-9801985319378542"
     data-ad-slot="2686695678"></ins>
<script>
     (adsbygoogle = window.adsbygoogle || []).push({});
</script>
        
  <div class="post-widgets">
    <div class="wp_rating">
      <div id="wpac-rating"></div>
    </div>
  </div>

        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/07/19/yisaiya5/" rel="prev" title="神的仆人成就的救恩 - 以赛亚书54">
      <i class="fa fa-chevron-left"></i> 神的仆人成就的救恩 - 以赛亚书54
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/26/yisaiya55/" rel="next" title="接受来自造物主的邀请 - 以赛亚书 55">
      接受来自造物主的邀请 - 以赛亚书 55 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>

          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Elements-of-RL"><span class="nav-number">1.</span> <span class="nav-text">Elements of RL</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#A-bit-history"><span class="nav-number">2.</span> <span class="nav-text">A bit history</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Simplest-problem-Multi-arm-Bandits"><span class="nav-number">3.</span> <span class="nav-text">Simplest problem: Multi-arm Bandits</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Action-Value-method"><span class="nav-number">3.1.</span> <span class="nav-text">Action Value method</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Incremental-Implementation"><span class="nav-number">3.2.</span> <span class="nav-text">Incremental Implementation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Nonstationary-Problem"><span class="nav-number">3.3.</span> <span class="nav-text">Nonstationary Problem</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Upper-Confidence-Bound-Action-Selection"><span class="nav-number">3.4.</span> <span class="nav-text">Upper-Confidence-Bound Action Selection</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Gradient-Bandits"><span class="nav-number">3.5.</span> <span class="nav-text">Gradient Bandits</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Different-Agents-of-Mult-arm-Bandits"><span class="nav-number">4.</span> <span class="nav-text">Different Agents of Mult-arm Bandits</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Random-Agent"><span class="nav-number">4.1.</span> <span class="nav-text">Random Agent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Greedy-Agent"><span class="nav-number">4.2.</span> <span class="nav-text">Greedy Agent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#epsilon-Greedy-Agent"><span class="nav-number">4.3.</span> <span class="nav-text">$\epsilon$-Greedy Agent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Reinforce-Agent"><span class="nav-number">4.4.</span> <span class="nav-text">Reinforce Agent</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Full-RL-Problem"><span class="nav-number">5.</span> <span class="nav-text">Full RL Problem</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Bandit-Env-Code-Attached"><span class="nav-number">6.</span> <span class="nav-text">Bandit Env Code Attached</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Zhe"
      src="/images/lambda.jpeg">
  <p class="site-author-name" itemprop="name">Zhe</p>
  <div class="site-description" itemprop="description">Things are correlated</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">86</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">62</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/wangzhe3224" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wangzhe3224" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:wangzhetju@gmail.com" title="E-Mail → mailto:wangzhetju@gmail.com" rel="noopener" target="_blank"><i class="fa fa-fw fa-envelope"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.co.uk/citations?hl=en&user=6yOk010AAAAJ" title="Google → https:&#x2F;&#x2F;scholar.google.co.uk&#x2F;citations?hl&#x3D;en&amp;user&#x3D;6yOk010AAAAJ" rel="noopener" target="_blank"><i class="fa fa-fw fa-google"></i></a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.zhihu.com/people/wangzhetju" title="StackOverflow → https:&#x2F;&#x2F;www.zhihu.com&#x2F;people&#x2F;wangzhetju" rel="noopener" target="_blank"><i class="fa fa-fw fa-stack-overflow"></i></a>
      </span>
  </div>


      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2020 – 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Zhe</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
      <span class="post-meta-item-text">Symbols count total: </span>
    <span title="Symbols count total">244k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
      <span class="post-meta-item-text">Reading time total &asymp;</span>
    <span title="Reading time total">3:42</span>
</div>

<span id="timeDate">Loding time...</span><span id="times">...</span>
<script>
    var now = new Date(); 
    function createtime() { 
        var grt= new Date("01/01/2020 00:00:00");//在此处修改你的建站时间，格式：月/日/年 时:分:秒
        now.setTime(now.getTime()+250); 
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days); 
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours); 
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum); 
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;} 
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum); 
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;} 
        document.getElementById("timeDate").innerHTML = "Running for "+dnum+" Days "; 
        document.getElementById("times").innerHTML = hnum + " Hour " + mnum + " Min " + snum + " Secs"; 
    } 
setInterval("createtime()",250);
</script>

        
<div class="busuanzi-count">
  <script pjax async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/theme-next/theme-next-pjax@0/pjax.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/gh/fancyapps/fancybox@3/dist/jquery.fancybox.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/lozad@1/dist/lozad.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

  <script>
var pjax = new Pjax({
  selectors: [
    'head title',
    '#page-configurations',
    '.content-wrap',
    '.post-toc-wrap',
    '.languages',
    '#pjax'
  ],
  switches: {
    '.post-toc-wrap': Pjax.switches.innerHTML
  },
  analytics: false,
  cacheBust: false,
  scrollTo : !CONFIG.bookmark.enable
});

window.addEventListener('pjax:success', () => {
  document.querySelectorAll('script[pjax], script#page-configurations, #pjax script').forEach(element => {
    var code = element.text || element.textContent || element.innerHTML || '';
    var parent = element.parentNode;
    parent.removeChild(element);
    var script = document.createElement('script');
    if (element.id) {
      script.id = element.id;
    }
    if (element.className) {
      script.className = element.className;
    }
    if (element.type) {
      script.type = element.type;
    }
    if (element.src) {
      script.src = element.src;
      // Force synchronous loading of peripheral JS.
      script.async = false;
    }
    if (element.getAttribute('pjax') !== null) {
      script.setAttribute('pjax', '');
    }
    if (code !== '') {
      script.appendChild(document.createTextNode(code));
    }
    parent.appendChild(script);
  });
  NexT.boot.refresh();
  // Define Motion Sequence & Bootstrap Motion.
  if (CONFIG.motion.enable) {
    NexT.motion.integrator
      .init()
      .add(NexT.motion.middleWares.subMenu)
      .add(NexT.motion.middleWares.postList)
      .bootstrap();
  }
  NexT.utils.updateSidebarPosition();
});
</script>




  



  <script pjax>
  if (CONFIG.page.isPost) {
    wpac_init = window.wpac_init || [];
    wpac_init.push({
      widget: 'Rating',
      id    : 30061,
      el    : 'wpac-rating',
      color : 'fc6423'
    });
    (function() {
      if ('WIDGETPACK_LOADED' in window) return;
      WIDGETPACK_LOADED = true;
      var mc = document.createElement('script');
      mc.type = 'text/javascript';
      mc.async = true;
      mc.src = '//embed.widgetpack.com/widget.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(mc, s.nextSibling);
    })();
  }
  </script>

  
<script src="/js/local-search.js"></script>













    <div id="pjax">
  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

<script>
  function loadCount() {
    var d = document, s = d.createElement('script');
    s.src = 'https://wangzhe.disqus.com/count.js';
    s.id = 'dsq-count-scr';
    (d.head || d.body).appendChild(s);
  }
  // defer loading until the whole page loading is completed
  window.addEventListener('load', loadCount, false);
</script>
<script>
  var disqus_config = function() {
    this.page.url = "http://yoursite.com/2020/07/25/rl1/";
    this.page.identifier = "2020/07/25/rl1/";
    this.page.title = "Reinforment Learning Introduction 1 - 2";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://wangzhe.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

    </div>
</body>
</html>
